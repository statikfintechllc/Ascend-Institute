import copy
import os
import time
import warnings
from functools import partial
from typing import Any, Callable

import httpx

from openhands.core.config import LLMConfig

with warnings.catch_warnings():
    warnings.simplefilter('ignore')
    import litellm

from litellm import ChatCompletionMessageToolCall, ModelInfo, PromptTokensDetails
from litellm import Message as LiteLLMMessage
from litellm import completion as litellm_completion
from litellm import completion_cost as litellm_completion_cost
from litellm.exceptions import RateLimitError
from litellm.types.utils import CostPerToken, ModelResponse, Usage
from litellm.utils import create_pretrained_tokenizer

from openhands.core.exceptions import LLMNoResponseError
from openhands.core.logger import openhands_logger as logger
from openhands.core.message import Message
from openhands.llm.debug_mixin import DebugMixin
from openhands.llm.fn_call_converter import (
    STOP_WORDS,
    convert_fncall_messages_to_non_fncall_messages,
    convert_non_fncall_messages_to_fncall_messages,
)
from openhands.llm.metrics import Metrics
from openhands.llm.retry_mixin import RetryMixin

__all__ = ['LLM']

LLM_RETRY_EXCEPTIONS: tuple[type[Exception], ...] = (
    RateLimitError,
    litellm.Timeout,
    litellm.InternalServerError,
    LLMNoResponseError,
)

CACHE_PROMPT_SUPPORTED_MODELS = [
    'claude-3-7-sonnet-20250219',
    'claude-3-5-sonnet-20241022',
    'claude-3-5-sonnet-20240620',
    'claude-3-5-haiku-20241022',
    'claude-3-haiku-20240307',
    'claude-3-opus-20240229',
]

FUNCTION_CALLING_SUPPORTED_MODELS = [
    'claude-3-7-sonnet-20250219',
    'claude-3-5-sonnet',
    'claude-3-5-sonnet-20240620',
    'claude-3-5-sonnet-20241022',
    'claude-3.5-haiku',
    'claude-3-5-haiku-20241022',
    'gpt-4o-mini',
    'gpt-4o',
    'o1-2024-12-17',
    'o3-mini-2025-01-31',
    'o3-mini',
    'o3',
    'o3-2025-04-16',
    'o4-mini',
    'o4-mini-2025-04-16',
    'gemini-2.5-pro',
    'gpt-4.1',
]

REASONING_EFFORT_SUPPORTED_MODELS = [
    'o1-2024-12-17',
    'o1',
    'o3',
    'o3-2025-04-16',
    'o3-mini-2025-01-31',
    'o3-mini',
    'o4-mini',
    'o4-mini-2025-04-16',
]

MODELS_WITHOUT_STOP_WORDS = [
    'o1-mini',
    'o1-preview',
    'o1',
    'o1-2024-12-17',
]

class LLM(RetryMixin, DebugMixin):
    def __init__(self, config: LLMConfig, metrics: Metrics | None = None, retry_listener: Callable[[int, int], None] | None = None):
        self._tried_model_info = False
        self.metrics: Metrics = metrics if metrics is not None else Metrics(model_name=config.model)
        self.cost_metric_supported: bool = True
        self.config: LLMConfig = copy.deepcopy(config)
        self.model_info: ModelInfo | None = None
        self.retry_listener = retry_listener

        if self.config.log_completions:
            if self.config.log_completions_folder is None:
                raise RuntimeError('log_completions_folder is required when log_completions is enabled')
            os.makedirs(self.config.log_completions_folder, exist_ok=True)

        with warnings.catch_warnings():
            warnings.simplefilter('ignore')
            self.init_model_info()

        if self.vision_is_active():
            logger.debug('LLM: model has vision enabled')
        if self.is_caching_prompt_active():
            logger.debug('LLM: caching prompt enabled')
        if self.is_function_calling_active():
            logger.debug('LLM: model supports function calling')

        if self.config.custom_tokenizer is not None:
            self.tokenizer = create_pretrained_tokenizer(self.config.custom_tokenizer)
        else:
            self.tokenizer = None

        kwargs: dict[str, Any] = {
            'temperature': self.config.temperature,
            'max_completion_tokens': self.config.max_output_tokens,
        }

        if (self.config.model.lower() in REASONING_EFFORT_SUPPORTED_MODELS or self.config.model.split('/')[-1] in REASONING_EFFORT_SUPPORTED_MODELS):
            kwargs['reasoning_effort'] = self.config.reasoning_effort
            kwargs.pop('temperature')

        if self.config.model.startswith('azure'):
            kwargs['max_tokens'] = self.config.max_output_tokens
            kwargs.pop('max_completion_tokens')

        self._completion = partial(
            litellm_completion,
            model=self.config.model,
            api_key=self.config.api_key.get_secret_value() if self.config.api_key else None,
            base_url=self.config.base_url,
            api_version=self.config.api_version,
            custom_llm_provider=self.config.custom_llm_provider,
            timeout=self.config.timeout,
            top_p=self.config.top_p,
            drop_params=self.config.drop_params,
            seed=self.config.seed,
            **kwargs,
        )

        self._completion_unwrapped = self._completion

        @self.retry_decorator(
            num_retries=self.config.num_retries,
            retry_exceptions=LLM_RETRY_EXCEPTIONS,
            retry_min_wait=self.config.retry_min_wait,
            retry_max_wait=self.config.retry_max_wait,
            retry_multiplier=self.config.retry_multiplier,
            retry_listener=self.retry_listener,
        )
        def wrapper(*args: Any, **kwargs: Any) -> Any:
            from openhands.io import json

            messages: list[dict[str, Any]] | dict[str, Any] = []
            mock_function_calling = not self.is_function_calling_active()

            if len(args) > 1:
                messages = args[1]
                kwargs['messages'] = messages
                args = args[2:]
            elif 'messages' in kwargs:
                messages = kwargs['messages']

            messages = messages if isinstance(messages, list) else [messages]
            original_fncall_messages = copy.deepcopy(messages)
            mock_fncall_tools = None

            if mock_function_calling and 'tools' in kwargs:
                messages = convert_fncall_messages_to_non_fncall_messages(messages, kwargs['tools'], add_in_context_learning_example=bool('openhands-lm' not in self.config.model))
                kwargs['messages'] = messages
                if self.config.model not in MODELS_WITHOUT_STOP_WORDS:
                    kwargs['stop'] = STOP_WORDS
                mock_fncall_tools = kwargs.pop('tools')
                if 'openhands-lm' in self.config.model:
                    kwargs['tool_choice'] = 'none'
                else:
                    kwargs.pop('tool_choice', None)

            if not messages:
                raise ValueError('The messages list is empty. At least one message is required.')

            self.log_prompt(messages)
            litellm.modify_params = self.config.modify_params
            if 'litellm_proxy' not in self.config.model:
                kwargs.pop('extra_body', None)

            start_time = time.time()
            logger.debug(f'LLM: calling litellm completion with model: {self.config.model}, base_url: {self.config.base_url}, args: {args}, kwargs: {kwargs}')
            resp: ModelResponse = self._completion_unwrapped(*args, **kwargs)
            latency = time.time() - start_time
            response_id = resp.get('id', 'unknown')
            self.metrics.add_response_latency(latency, response_id)

            non_fncall_response = copy.deepcopy(resp)

            if mock_function_calling and mock_fncall_tools is not None:
                if len(resp.choices) < 1:
                    raise LLMNoResponseError('Response choices is less than 1 - This is only seen in Gemini models so far. Response: ' + str(resp))
                non_fncall_response_message = resp.choices[0].message
                fn_call_messages_with_response = convert_non_fncall_messages_to_fncall_messages(messages + [non_fncall_response_message], mock_fncall_tools)
                fn_call_response_message = fn_call_messages_with_response[-1]
                if not isinstance(fn_call_response_message, LiteLLMMessage):
                    fn_call_response_message = LiteLLMMessage(**fn_call_response_message)
                resp.choices[0].message = fn_call_response_message

            if not resp.get('choices') or len(resp['choices']) < 1:
                raise LLMNoResponseError('Response choices is less than 1 - Response: ' + str(resp))

            message_back: str = resp['choices'][0]['message']['content'] or ''
            tool_calls: list[ChatCompletionMessageToolCall] = resp['choices'][0]['message'].get('tool_calls', [])
            if tool_calls:
                for tool_call in tool_calls:
                    fn_name = tool_call.function.name
                    fn_args = tool_call.function.arguments
                    message_back += f'\nFunction call: {fn_name}({fn_args})'

            self.log_response(message_back)
            cost = self._post_completion(resp)

            if self.config.log_completions:
                assert self.config.log_completions_folder is not None
                log_file = os.path.join(self.config.log_completions_folder, f'{self.metrics.model_name.replace("/", "__")}-{time.time()}.json')
                _d = {
                    'messages': messages,
                    'response': resp,
                    'args': args,
                    'kwargs': {k: v for k, v in kwargs.items() if k not in ('messages', 'client')},
                    'timestamp': time.time(),
                    'cost': cost,
                }
                if mock_function_calling:
                    _d['response'] = non_fncall_response
                    _d['fncall_messages'] = original_fncall_messages
                    _d['fncall_response'] = resp
                with open(log_file, 'w') as f:
                    f.write(json.dumps(_d))

            return resp

        self._completion = wrapper

    # ... TRUNCATED for readability ... you still have rest of the methods (like format_messages, etc) exactly as you posted.

    def init_model_info(self) -> None:
        if self._tried_model_info:
            return
        self._tried_model_info = True

        if self._is_local():
            logger.debug(f"Detected local model at {self.config.base_url}, setting manual model_info.")
            self.model_info = {
                'max_input_tokens': 32768,
                'max_output_tokens': 4096,
                'supports_vision': False,
            }
            self._function_calling_active = False
            return

        try:
            if self.config.model.startswith('openrouter'):
                self.model_info = litellm.get_model_info(self.config.model)
        except Exception as e:
            logger.debug(f'Error getting model info: {e}')

        # (your old stuff here continues)

    def _is_local(self) -> bool:
        base_url = self.config.base_url or ''
        model = self.config.model or ''
        return any(s in base_url for s in ['localhost', '127.0.0.1', '0.0.0.0']) or model.startswith('ollama')

# END OF PATCH ðŸ”¥
